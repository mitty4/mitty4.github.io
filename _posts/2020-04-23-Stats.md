---
published: true
---
## ReLU (rectified linear unit) Activation Function
### For Neural Networks

##### 05/03/2020
**https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6

ReLU is the most common activation function as of now because it is used in just about every convolutional neural networks.

<img src="{{ site.baseurl }}/images/relu.png" alt="sigmoidal function by https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" style="width: 400px;"/>

The range is from 0 to infinity. Because the graph is split, the negative values always graph to 0, which can be an issue considering that no negative values will show on the graph.


## Hyperbolic or Tanh Activation Function
### For Neural Networks

##### 05/03/2020
**https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6

Hyperbolic functions are s-shaped just like sigmoidal, however, the range for these is -1 to 1, which makes this function typical for classification between two classes.

<img src="{{ site.baseurl }}/images/tanh.jpeg" alt="sigmoidal function by https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" style="width: 400px;"/>

Both tanh and sigmoidal functions are used for feed forward nets.

## Sigmoidal Activation Functions
### For Neural Networks

##### 05/02/2020
**https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6

Let's make this simple from the start: sigmoid is synonymous with logistic and it exists between 0 and 1, which makes it ideal for probability functions because the range for probability exists between 0 and 1!

<img src="{{ site.baseurl }}/images/sigmoidal.png" alt="sigmoidal function by https://towardsdatascience.com/activation-functions-neural-networks-1cbd9f8d91d6" style="width: 400px;"/>


## Equations and stats standards

Capital sigma (Î£) represents a series.
